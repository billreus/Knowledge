## 3.1.Urllib

共有四个模块：

1. 模块request，是HTTP请求模块，模拟发送请求（浏览器输入网址敲击回车）

2. 模块erro，异常处理模块，如果出现请求错误，可以捕获异常，再进行重试。

3. 模块parse，工具模块，提供许多URL处理方法，如拆分解析合并。

4. 模块robotparser，用于识别网站robots.txt文件，判断哪些网站可以爬取。

### 3.1.1 发送请求

使用Urllib的request模块可以方便的实现request的发送并得到response。

urllib.request模块可以模拟浏览器的一个请求发起过程。

#### urlopen()

```py
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(response.read().decode('utf-8'))
```

该代码即完成了对python官网的网页源代码的抓取，完成了网页的GET请求。

POST请求需要添加数据，以bytes类型传递到data内。

```py
import urllib.parse
import urllib.request

data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read())
```

对于超时可以设置timeout参数，超出请求时间抛出异常。

```py
import socket
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

当时间超过0.1s时，error模块抛出异常。

urlopen()函数的API`urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`

#### Requset

Request可以通过参数来构造：
`class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`
url为请求的URL；data是bytes类型的传递参数；headers参数是一个字典，可以直接构造，也可以调用add_header()来添加。

* 添加Request Headers最常用的方法是修改User-Agent来伪装浏览器，默认agent是Python-urllib,如果要伪装成火狐，可以设置为`Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11`

origin_req_host 参数指的是请求方的 host 名称或者 IP 地址;

unverifiable 参数指的是这个请求是否是无法验证的，默认是False。意思就是说用户没有足够权限来选择接收这个请求的结果;

method 参数是一个字符串，它用来指示请求使用的方法，比如GET，POST，PUT等等。

```py
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```

#### 高级用法

在处理Cookies，代理设置等操作，需要Handler。

urllib.request 模块里的 BaseHandler类，它是所有其他 Handler 的父类，它提供了最基本的 Handler 的方法。

另外一个比较重要的类就是 OpenerDirector，我们可以称之为 Opener，我们之前用过 urlopen() 这个方法，实际上它就是 Urllib为我们提供的一个 Opener。

例如需要输入用户名密码登陆时

```py
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = 'username'
password = 'password'
url = 'http://localhost:5000/'

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)

try:
    result = opener.open(url)
    html = result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```

在这里，首先实例化了一个 HTTPBasicAuthHandler 对象，参数是 HTTPPasswordMgrWithDefaultRealm 对象，它利用 add_password() 添加进去用户名和密码，这样我们就建立了一个处理认证的 Handler。
接下来利用 build_opener() 方法来利用这个 Handler 构建一个 Opener，那么这个 Opener 在发送请求的时候就相当于已经认证成功了。
接下来利用 Opener 的 open() 方法打开链接，就可以完成认证了，在这里获取到的结果就是认证后的页面源码内容。

#### 代理

```py
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy_handler = ProxyHandler({
    'http': 'http://127.0.0.1:9743',
    'https': 'https://127.0.0.1:9743'
})
opener = build_opener(proxy_handler)
try:
    response = opener.open('https://www.baidu.com')
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```

在本地搭建一个代理，运行在9743端口上。

在这里使用了 ProxyHandler，ProxyHandler 的参数是一个字典，键名是协议类型，比如 HTTP 还是 HTTPS 等，键值是代理链接，可以添加多个代理。
然后利用 build_opener() 方法利用这个 Handler 构造一个 Opener，然后发送请求即可。

#### Cookies

Cookies处理需要Cookies相关的Handler

```py
import http.cookiejar, urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name+"="+item.value)
```

即可得到每一条Coolie的名称还有值。

如果要输出成文件

```py
filename = 'cookies.txt'
cookie = http.cookiejar.LWPCookieJar(filename)
#cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

使用文件时

```py
cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
print(response.read().decode('utf-8'))
```

即可输出百度网页的源代码。

### 3.1.2 异常处理

#### URLError

URLError类是Urllib库的error模块，具有一个reason属性，可以返回错误原因。

```py
from urllib import request, error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
```

#### HTTPError

HTTPError是URLError的子类用来处理HTTP请求错误。

有三个属性：

* code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。
* reason，同父类一样，返回错误的原因。
* headers，返回 Request Headers。

```py
from urllib import request, error

try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successfully')
```

先捕获 HTTPError，获取它的错误状态码、原因、Headers 等详细信息。如果非 HTTPError，再捕获 URLError 异常，输出错误原因。

### 3.1.3 解析链接

parse模块，定义了处理URL的接口，实现URL各部分的抽取，合并和链接转换。

#### urlparse()

urlparse()可以实现对URL的识别和分段。

```py
from urllib.parse import urlparse

result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
print(type(result), result)

>>> <class 'urllib.parse.ParseResult'>
>>> ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
```

返回结果是一个 ParseResult 类型的对象，它包含了六个部分，分别是 scheme、netloc、path、params、query、fragment。根据每个部分后面的符号识别。

上面是最基本的解析方式，API为：`urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)`
urlstring是待解析的URL，scheme是默认协议（如http、https等）且scheme 参数只有在 URL 中不包含 scheme 信息时才会生效。

allow_fragments，即是否忽略 fragment，如果它被设置为 False，fragment 部分就会被忽略，它会被解析为 path、parameters 或者 query 的一部分，fragment 部分为空。

#### urlunparse()

为urlparse()对立的方法，接受参数是一个可迭代对象，但长度必须是6。

```py
from urllib.parse import urlunparse

data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
```

#### urlsplit()

与urlparse()方法相似，不过不会单独解析parameters，只返回五个结果。

```py
from urllib.parse import urlsplit

result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
print(result)
print(result.scheme, result[0])

>>>SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')
>>>http http
```

#### urlunsplit()

与 urlunparse() 类似，也是将链接的各个部分组合成完整链接的方法,长度必须是5.

```py
from urllib.parse import urlunsplit

data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']
print(urlunsplit(data))

>>>http://www.baidu.com/index.html?a=6#comment
```

#### urljoin()

有了 urlunparse() 和 urlunsplit() 方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。

生成链接还有另一个方法，利用 urljoin() 方法我们可以提供一个 base_url（基础链接），新的链接作为第二个参数，方法会分析 base_url 的 scheme、netloc、path 这三个内容对新链接缺失的部分进行补充，作为结果返回。

```py
from urllib.parse import urljoin

print(urljoin('http://www.baidu.com', 'FAQ.html'))
print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))
print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
print(urljoin('http://www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com#comment', '?category=2'))

>>>http://www.baidu.com/FAQ.html
   https://cuiqingcai.com/FAQ.html
   https://cuiqingcai.com/FAQ.html
   https://cuiqingcai.com/FAQ.html?question=2
   https://cuiqingcai.com/index.php
   http://www.baidu.com?category=2#comment
   www.baidu.com?category=2#comment
   www.baidu.com?category=2
```

#### urlencode()

可以将字典类型转换为GET请求参数

```py
from urllib.parse import urlencode

params = {
    'name': 'germey',
    'age': 22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)

>>>http://www.baidu.com?name=germey&age=22
```

声明了一个字典，将参数表示出来，然后调用 urlencode() 方法将其序列化为 URL 标准 GET 请求参数。

#### parse_qs()

如果我们有一串 GET 请求参数，我们利用 parse_qs() 方法就可以将它转回字典

```py
from urllib.parse import parse_qs

query = 'name=germey&age=22'
print(parse_qs(query))

>>>{'name': ['germey'], 'age': ['22']}
```

#### parse_qsl()

将参数转化为元组组成的列表

```py
from urllib.parse import parse_qsl

query = 'name=germey&age=22'
print(parse_qsl(query))

>>>[('name', 'germey'), ('age', '22')]
```

#### quote()

quote() 方法可以将内容转化为 URL 编码的格式，有时候 URL 中带有中文参数的时候可能导致乱码的问题，所以我们可以用这个方法将中文字符转化为 URL 编码.

```py
from urllib.parse import quote

keyword = '壁纸'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
print(url)

>>>https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
```

#### unquote()

可以进行 URL 解码

```py
from urllib.parse import unquote

url = 'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
print(unquote(url))

>>>https://www.baidu.com/s?wd=壁纸
```

### 3.1.4 Robots协议

该协议用来告诉爬虫和搜索引擎什么页面可以抓取，什么页面不可抓取，通常放在一个robots.txt的文本文件中。

#### robotparser

robotparser模块中RobotFileParser类用来解析robots.txt文件判断是否有权限爬取

```py
from urllib.robotparser import RobotFileParser

rp = RobotFileParser()
rp.set_url('http://www.jianshu.com/robots.txt')
# rp = RobotFileParser('http://www.jianshu.com/robots.txt')
rp.read()
print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))

>>>True False
```

* set_url()用于设置robots.txt文件的连接。
* read()，读取robots.txt文件并进行 分析
* can_fetch()，传入两个参数，一是User-agent,二是URL，返回该搜索引擎是能抓取这个URL，返回结果是True或False

## 3.2 Requests

### 3.2.1 基本使用

在Urllib库中request模块使用urlopen()来模拟get请求网页。

在requests中相对应的方法就是get()

```py
import requests

r = requests.get('https://www.baidu.com/')
print(type(r))
print(r.status_code)
print(type(r.text))
print(r.text)
print(r.cookies)
```

对于post()、put()、delete()、haed()、options()等方法也可以像get()一样调用。

#### GET请求

如果GET请求要附加一些信息，可以使用params参数

```py
import requests

data = {
    'name': 'germey',
    'age': 22
}
r = requests.get("http://httpbin.org/get", params=data)
print(r.text)
```

另外，网页的返回类型实际上是 str 类型，但是它很特殊，是 Json 的格式，所以如果我们想直接把返回结果解析，得到一个字典格式的话，可以直接调用 json() 方法。

```py
import requests

r = requests.get("http://httpbin.org/get")
print(type(r.text))
print(r.json())
print(type(r.json()))

>>><class 'str'>
    {'headers': {'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.10.0'}, 'url': 'http://httpbin.org/get', 'args': {}, 'origin': '182.33.248.131'}
    <class 'dict'>
```

#### 抓取网页

爬取知乎发现页：

```py
import requests
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
pattern = re.compile('explore-feed.*?question_link.*?>(.*?)</a>', re.S)
titles = re.findall(pattern, r.text)
print(titles)
```

这里加入了Headers信息，包含了User-Agent字段信息，即浏览器标识，如果不加，知乎会禁止爬取。

#### 抓取二进制

上面抓取返回的实际是一个HTML文档，如果我们想抓取图片、音频、视频等文件，由于这些本质是由二进制码组成的，所以要拿到二进制码才能爬取。

例如爬取github站点图标：

```py
import requests

r = requests.get("https://github.com/favicon.ico")
print(r.text) # 站点名
print(r.content) # 图标
```

将图片保存

```py
import requests

r = requests.get("https://github.com/favicon.ico")
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```

在这里用了 open() 方法，第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据，然后保存。

#### Response

在上面我们使用text和content获取了Response内容，不过还有StatusCode、Headers、Cookies等信息。

```py
import requests

r = requests.get('http://www.jianshu.com')
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```

在这里 Status Code 常用来判断请求是否成功，Requests 还提供了一个内置的 Status Code 查询对象 requests.codes。

```py
import requests

r = requests.get('http://www.jianshu.com')
exit() if not r.status_code == requests.codes.ok else print('Request Successfully')
```

在这里我们用 requests.codes.ok 得到的是成功的状态码200.当然成功的条件不止这一个码。