## 3.1.Urllib

共有四个模块：

1. 模块request，是HTTP请求模块，模拟发送请求（浏览器输入网址敲击回车）

2. 模块erro，异常处理模块，如果出现请求错误，可以捕获异常，再进行重试。

3. 模块parse，工具模块，提供许多URL处理方法，如拆分解析合并。

4. 模块robotparser，用于识别网站robots.txt文件，判断哪些网站可以爬取。

### 3.1.1 发送请求

使用Urllib的request模块可以方便的实现request的发送并得到response。

urllib.request模块可以模拟浏览器的一个请求发起过程。

#### urlopen()

```py
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(response.read().decode('utf-8'))
```

该代码即完成了对python官网的网页源代码的抓取，完成了网页的GET请求。

POST请求需要添加数据，以bytes类型传递到data内。

```py
import urllib.parse
import urllib.request

data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read())
```

对于超时可以设置timeout参数，超出请求时间抛出异常。

```py
import socket
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

当时间超过0.1s时，error模块抛出异常。

urlopen()函数的API`urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`

#### Requset

Request可以通过参数来构造：
`class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`
url为请求的URL；data是bytes类型的传递参数；headers参数是一个字典，可以直接构造，也可以调用add_header()来添加。

* 添加Request Headers最常用的方法是修改User-Agent来伪装浏览器，默认agent是Python-urllib,如果要伪装成火狐，可以设置为`Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11`

origin_req_host 参数指的是请求方的 host 名称或者 IP 地址;

unverifiable 参数指的是这个请求是否是无法验证的，默认是False。意思就是说用户没有足够权限来选择接收这个请求的结果;

method 参数是一个字符串，它用来指示请求使用的方法，比如GET，POST，PUT等等。

```py
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```

#### 高级用法

在处理Cookies，代理设置等操作，需要Handler。

urllib.request 模块里的 BaseHandler类，它是所有其他 Handler 的父类，它提供了最基本的 Handler 的方法。

另外一个比较重要的类就是 OpenerDirector，我们可以称之为 Opener，我们之前用过 urlopen() 这个方法，实际上它就是 Urllib为我们提供的一个 Opener。

例如需要输入用户名密码登陆时

```py
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = 'username'
password = 'password'
url = 'http://localhost:5000/'

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)

try:
    result = opener.open(url)
    html = result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```

在这里，首先实例化了一个 HTTPBasicAuthHandler 对象，参数是 HTTPPasswordMgrWithDefaultRealm 对象，它利用 add_password() 添加进去用户名和密码，这样我们就建立了一个处理认证的 Handler。
接下来利用 build_opener() 方法来利用这个 Handler 构建一个 Opener，那么这个 Opener 在发送请求的时候就相当于已经认证成功了。
接下来利用 Opener 的 open() 方法打开链接，就可以完成认证了，在这里获取到的结果就是认证后的页面源码内容。

#### 代理

```py
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy_handler = ProxyHandler({
    'http': 'http://127.0.0.1:9743',
    'https': 'https://127.0.0.1:9743'
})
opener = build_opener(proxy_handler)
try:
    response = opener.open('https://www.baidu.com')
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```

在本地搭建一个代理，运行在9743端口上。

在这里使用了 ProxyHandler，ProxyHandler 的参数是一个字典，键名是协议类型，比如 HTTP 还是 HTTPS 等，键值是代理链接，可以添加多个代理。
然后利用 build_opener() 方法利用这个 Handler 构造一个 Opener，然后发送请求即可。

#### Cookies

Cookies处理需要Cookies相关的Handler

```py
import http.cookiejar, urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name+"="+item.value)
```

即可得到每一条Coolie的名称还有值。

如果要输出成文件

```py
filename = 'cookies.txt'
cookie = http.cookiejar.LWPCookieJar(filename)
#cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

使用文件时

```py
cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
print(response.read().decode('utf-8'))
```

即可输出百度网页的源代码。
