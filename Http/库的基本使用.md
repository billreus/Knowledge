<!-- TOC -->

- [3.1.Urllib](#31urllib)
    - [3.1.1 发送请求](#311-发送请求)
        - [urlopen()](#urlopen)
        - [Requset](#requset)
        - [高级用法](#高级用法)
        - [代理](#代理)
        - [Cookies](#cookies)
    - [3.1.2 异常处理](#312-异常处理)
        - [URLError](#urlerror)
        - [HTTPError](#httperror)
    - [3.1.3 解析链接](#313-解析链接)
        - [urlparse()](#urlparse)
        - [urlunparse()](#urlunparse)
        - [urlsplit()](#urlsplit)
        - [urlunsplit()](#urlunsplit)
        - [urljoin()](#urljoin)
        - [urlencode()](#urlencode)
        - [parse_qs()](#parse_qs)
        - [parse_qsl()](#parse_qsl)
        - [quote()](#quote)
        - [unquote()](#unquote)
    - [3.1.4 Robots协议](#314-robots协议)
        - [robotparser](#robotparser)
- [3.2 Requests](#32-requests)
    - [3.2.1 基本使用](#321-基本使用)
        - [GET请求](#get请求)
        - [抓取网页](#抓取网页)
        - [抓取二进制](#抓取二进制)
        - [Response](#response)
    - [3.2.2 高级用法](#322-高级用法)
        - [文件上传](#文件上传)
        - [cookies](#cookies)
        - [会话维持](#会话维持)
        - [SSL证书验证](#ssl证书验证)
        - [代理设置](#代理设置)
        - [超时设置](#超时设置)
        - [身份验证](#身份验证)
        - [Prepared Request](#prepared-request)
- [3.3 正则表达式](#33-正则表达式)
    - [3.3.1 match()](#331-match)
        - [匹配](#匹配)
        - [通用匹配](#通用匹配)
        - [贪婪与非贪婪](#贪婪与非贪婪)
        - [修饰符](#修饰符)
        - [转义匹配](#转义匹配)
    - [3.3.2 search()](#332-search)
    - [3.3.3 findall()](#333-findall)
    - [3.3.4 sub()](#334-sub)
    - [3.3.5 compile()](#335-compile)

<!-- /TOC -->

## 3.1.Urllib

共有四个模块：

1. 模块request，是HTTP请求模块，模拟发送请求（浏览器输入网址敲击回车）

2. 模块erro，异常处理模块，如果出现请求错误，可以捕获异常，再进行重试。

3. 模块parse，工具模块，提供许多URL处理方法，如拆分解析合并。

4. 模块robotparser，用于识别网站robots.txt文件，判断哪些网站可以爬取。

### 3.1.1 发送请求

使用Urllib的request模块可以方便的实现request的发送并得到response。

urllib.request模块可以模拟浏览器的一个请求发起过程。

#### urlopen()

```py
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(response.read().decode('utf-8'))
```

该代码即完成了对python官网的网页源代码的抓取，完成了网页的GET请求。

POST请求需要添加数据，以bytes类型传递到data内。

```py
import urllib.parse
import urllib.request

data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read())
```

对于超时可以设置timeout参数，超出请求时间抛出异常。

```py
import socket
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

当时间超过0.1s时，error模块抛出异常。

urlopen()函数的API`urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`

#### Requset

Request可以通过参数来构造：
`class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`
url为请求的URL；data是bytes类型的传递参数；headers参数是一个字典，可以直接构造，也可以调用add_header()来添加。

* 添加Request Headers最常用的方法是修改User-Agent来伪装浏览器，默认agent是Python-urllib,如果要伪装成火狐，可以设置为`Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11`

origin_req_host 参数指的是请求方的 host 名称或者 IP 地址;

unverifiable 参数指的是这个请求是否是无法验证的，默认是False。意思就是说用户没有足够权限来选择接收这个请求的结果;

method 参数是一个字符串，它用来指示请求使用的方法，比如GET，POST，PUT等等。

```py
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```

#### 高级用法

在处理Cookies，代理设置等操作，需要Handler。

urllib.request 模块里的 BaseHandler类，它是所有其他 Handler 的父类，它提供了最基本的 Handler 的方法。

另外一个比较重要的类就是 OpenerDirector，我们可以称之为 Opener，我们之前用过 urlopen() 这个方法，实际上它就是 Urllib为我们提供的一个 Opener。

例如需要输入用户名密码登陆时

```py
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = 'username'
password = 'password'
url = 'http://localhost:5000/'

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)

try:
    result = opener.open(url)
    html = result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```

在这里，首先实例化了一个 HTTPBasicAuthHandler 对象，参数是 HTTPPasswordMgrWithDefaultRealm 对象，它利用 add_password() 添加进去用户名和密码，这样我们就建立了一个处理认证的 Handler。
接下来利用 build_opener() 方法来利用这个 Handler 构建一个 Opener，那么这个 Opener 在发送请求的时候就相当于已经认证成功了。
接下来利用 Opener 的 open() 方法打开链接，就可以完成认证了，在这里获取到的结果就是认证后的页面源码内容。

#### 代理

```py
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy_handler = ProxyHandler({
    'http': 'http://127.0.0.1:9743',
    'https': 'https://127.0.0.1:9743'
})
opener = build_opener(proxy_handler)
try:
    response = opener.open('https://www.baidu.com')
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```

在本地搭建一个代理，运行在9743端口上。

在这里使用了 ProxyHandler，ProxyHandler 的参数是一个字典，键名是协议类型，比如 HTTP 还是 HTTPS 等，键值是代理链接，可以添加多个代理。
然后利用 build_opener() 方法利用这个 Handler 构造一个 Opener，然后发送请求即可。

#### Cookies

Cookies处理需要Cookies相关的Handler

```py
import http.cookiejar, urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name+"="+item.value)
```

即可得到每一条Coolie的名称还有值。

如果要输出成文件

```py
filename = 'cookies.txt'
cookie = http.cookiejar.LWPCookieJar(filename)
#cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

使用文件时

```py
cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
print(response.read().decode('utf-8'))
```

即可输出百度网页的源代码。

### 3.1.2 异常处理

#### URLError

URLError类是Urllib库的error模块，具有一个reason属性，可以返回错误原因。

```py
from urllib import request, error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
```

#### HTTPError

HTTPError是URLError的子类用来处理HTTP请求错误。

有三个属性：

* code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。
* reason，同父类一样，返回错误的原因。
* headers，返回 Request Headers。

```py
from urllib import request, error

try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successfully')
```

先捕获 HTTPError，获取它的错误状态码、原因、Headers 等详细信息。如果非 HTTPError，再捕获 URLError 异常，输出错误原因。

### 3.1.3 解析链接

parse模块，定义了处理URL的接口，实现URL各部分的抽取，合并和链接转换。

#### urlparse()

urlparse()可以实现对URL的识别和分段。

```py
from urllib.parse import urlparse

result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
print(type(result), result)

>>> <class 'urllib.parse.ParseResult'>
>>> ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
```

返回结果是一个 ParseResult 类型的对象，它包含了六个部分，分别是 scheme、netloc、path、params、query、fragment。根据每个部分后面的符号识别。

上面是最基本的解析方式，API为：`urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)`
urlstring是待解析的URL，scheme是默认协议（如http、https等）且scheme 参数只有在 URL 中不包含 scheme 信息时才会生效。

allow_fragments，即是否忽略 fragment，如果它被设置为 False，fragment 部分就会被忽略，它会被解析为 path、parameters 或者 query 的一部分，fragment 部分为空。

#### urlunparse()

为urlparse()对立的方法，接受参数是一个可迭代对象，但长度必须是6。

```py
from urllib.parse import urlunparse

data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
```

#### urlsplit()

与urlparse()方法相似，不过不会单独解析parameters，只返回五个结果。

```py
from urllib.parse import urlsplit

result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
print(result)
print(result.scheme, result[0])

>>>SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')
>>>http http
```

#### urlunsplit()

与 urlunparse() 类似，也是将链接的各个部分组合成完整链接的方法,长度必须是5.

```py
from urllib.parse import urlunsplit

data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']
print(urlunsplit(data))

>>>http://www.baidu.com/index.html?a=6#comment
```

#### urljoin()

有了 urlunparse() 和 urlunsplit() 方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。

生成链接还有另一个方法，利用 urljoin() 方法我们可以提供一个 base_url（基础链接），新的链接作为第二个参数，方法会分析 base_url 的 scheme、netloc、path 这三个内容对新链接缺失的部分进行补充，作为结果返回。

```py
from urllib.parse import urljoin

print(urljoin('http://www.baidu.com', 'FAQ.html'))
print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))
print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
print(urljoin('http://www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com#comment', '?category=2'))

>>>http://www.baidu.com/FAQ.html
   https://cuiqingcai.com/FAQ.html
   https://cuiqingcai.com/FAQ.html
   https://cuiqingcai.com/FAQ.html?question=2
   https://cuiqingcai.com/index.php
   http://www.baidu.com?category=2#comment
   www.baidu.com?category=2#comment
   www.baidu.com?category=2
```

#### urlencode()

可以将字典类型转换为GET请求参数

```py
from urllib.parse import urlencode

params = {
    'name': 'germey',
    'age': 22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)

>>>http://www.baidu.com?name=germey&age=22
```

声明了一个字典，将参数表示出来，然后调用 urlencode() 方法将其序列化为 URL 标准 GET 请求参数。

#### parse_qs()

如果我们有一串 GET 请求参数，我们利用 parse_qs() 方法就可以将它转回字典

```py
from urllib.parse import parse_qs

query = 'name=germey&age=22'
print(parse_qs(query))

>>>{'name': ['germey'], 'age': ['22']}
```

#### parse_qsl()

将参数转化为元组组成的列表

```py
from urllib.parse import parse_qsl

query = 'name=germey&age=22'
print(parse_qsl(query))

>>>[('name', 'germey'), ('age', '22')]
```

#### quote()

quote() 方法可以将内容转化为 URL 编码的格式，有时候 URL 中带有中文参数的时候可能导致乱码的问题，所以我们可以用这个方法将中文字符转化为 URL 编码.

```py
from urllib.parse import quote

keyword = '壁纸'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
print(url)

>>>https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
```

#### unquote()

可以进行 URL 解码

```py
from urllib.parse import unquote

url = 'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
print(unquote(url))

>>>https://www.baidu.com/s?wd=壁纸
```

### 3.1.4 Robots协议

该协议用来告诉爬虫和搜索引擎什么页面可以抓取，什么页面不可抓取，通常放在一个robots.txt的文本文件中。

#### robotparser

robotparser模块中RobotFileParser类用来解析robots.txt文件判断是否有权限爬取

```py
from urllib.robotparser import RobotFileParser

rp = RobotFileParser()
rp.set_url('http://www.jianshu.com/robots.txt')
# rp = RobotFileParser('http://www.jianshu.com/robots.txt')
rp.read()
print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))

>>>True False
```

* set_url()用于设置robots.txt文件的连接。
* read()，读取robots.txt文件并进行 分析
* can_fetch()，传入两个参数，一是User-agent,二是URL，返回该搜索引擎是能抓取这个URL，返回结果是True或False

## 3.2 Requests

### 3.2.1 基本使用

在Urllib库中request模块使用urlopen()来模拟get请求网页。

在requests中相对应的方法就是get()

```py
import requests

r = requests.get('https://www.baidu.com/')
print(type(r))
print(r.status_code)
print(type(r.text))
print(r.text)
print(r.cookies)
```

对于post()、put()、delete()、haed()、options()等方法也可以像get()一样调用。

#### GET请求

如果GET请求要附加一些信息，可以使用params参数

```py
import requests

data = {
    'name': 'germey',
    'age': 22
}
r = requests.get("http://httpbin.org/get", params=data)
print(r.text)
```

另外，网页的返回类型实际上是 str 类型，但是它很特殊，是 Json 的格式，所以如果我们想直接把返回结果解析，得到一个字典格式的话，可以直接调用 json() 方法。

```py
import requests

r = requests.get("http://httpbin.org/get")
print(type(r.text))
print(r.json())
print(type(r.json()))

>>><class 'str'>
    {'headers': {'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.10.0'}, 'url': 'http://httpbin.org/get', 'args': {}, 'origin': '182.33.248.131'}
    <class 'dict'>
```

#### 抓取网页

爬取知乎发现页：

```py
import requests
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
pattern = re.compile('explore-feed.*?question_link.*?>(.*?)</a>', re.S)
titles = re.findall(pattern, r.text)
print(titles)
```

这里加入了Headers信息，包含了User-Agent字段信息，即浏览器标识，如果不加，知乎会禁止爬取。

#### 抓取二进制

上面抓取返回的实际是一个HTML文档，如果我们想抓取图片、音频、视频等文件，由于这些本质是由二进制码组成的，所以要拿到二进制码才能爬取。

例如爬取github站点图标：

```py
import requests

r = requests.get("https://github.com/favicon.ico")
print(r.text) # 站点名
print(r.content) # 图标
```

将图片保存

```py
import requests

r = requests.get("https://github.com/favicon.ico")
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```

在这里用了 open() 方法，第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据，然后保存。

#### Response

在上面我们使用text和content获取了Response内容，不过还有StatusCode、Headers、Cookies等信息。

```py
import requests

r = requests.get('http://www.jianshu.com')
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```

在这里 Status Code 常用来判断请求是否成功，Requests 还提供了一个内置的 Status Code 查询对象 requests.codes。

```py
import requests

r = requests.get('http://www.jianshu.com')
exit() if not r.status_code == requests.codes.ok else print('Request Successfully')
```

在这里我们用 requests.codes.ok 得到的是成功的状态码200.当然成功的条件不止这一个码。

### 3.2.2 高级用法

Requests的一些高级用法，如文件上传，代理设置，cookies设置等

#### 文件上传

如果网址需要我们上传文件，可以使用requests.post

```py
import requests

files = {'file': open('favicon.ico', 'rb')}
r = requests.post('http://httpbin.org/post', files=files)
print(r.text)
```

#### cookies

Urllib中处理cookies过于复杂，requests只需要一步：

```py
import requests

r = requests.get('https://www.baidu.com')
print(r.cookies)
for key, value in r.cookies.items():
    print(key + '=' + value)
```

首先我们调用了 cookies 属性即可成功得到了 Cookies，可以发现它是一个 RequestCookieJar 类型，然后我们用 items() 方法将其转化为元组组成的列表，遍历输出每一个 Cookie 的名和值，实现 Cookies 的遍历解析

#### 会话维持

在requests中，我们直接利用get()或post()等方法模拟网页请求，实际上相当于不同的会话，等同于同时打开多个网页。

想保持会话，又不想每次设置cookies，可以使用session对象：

```py
import requests

s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
r = s.get('http://httpbin.org/cookies')
print(r.text)

>>> {
      "cookies": {
        "number": "123456789"
      }
    }
```

测试中我们向网址请求设置了一个coookies，然后请求get得到当前cookies。

#### SSL证书验证

Requests 提供了证书验证的功能，当发送 HTTP 请求的时候，它会检查 SSL 证书，我们可以使用 verify 这个参数来控制是否检查此证书，其实如果不加的话默认是 True，会自动验证。

用requests请求会出现以下问题：

```py
import requests

response = requests.get('https://www.12306.cn')
# response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)

>>>requests.exceptions.SSLError: ("bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)",)
```

提示一个错误，叫做 SSLError，证书验证错误。所以如果我们请求一个 HTTPS 站点，但是证书验证错误的页面时，就会报这样的错误，那么如何避免这个错误呢？很简单，把 verify 这个参数设置为 False 即可。

#### 代理设置

有些网址大规模爬取时会进行验证，我们需要使用代理来解决，即requests中proxies这个参数。

 ```py
import requests

proxies = {
  'http': 'http://10.10.1.10:3128',
  'https': 'http://10.10.1.10:1080',
}

requests.get('https://www.taobao.com', proxies=proxies)
 ```

当然直接运行这个实例可能不行，因为这个代理可能是无效的，请换成自己的有效代理试验一下。

若代理需要使用 HTTP Basic Auth，可以使用类似 http://user:password@host:port 这样的语法来设置代理。

```py
import requests

proxies = {
    'https': 'http://user:password@10.10.1.10:3128/',
}
requests.get('https://www.taobao.com', proxies=proxies)
```

除了HTTP代理，还可以使用SOCKS协议处理：

```py
import requests

proxies = {
    'http': 'socks5://user:password@host:port',
    'https': 'socks5://user:password@host:port'
}
requests.get('https://www.taobao.com', proxies=proxies)
```

#### 超时设置

设置超时时间需要用到 timeout 参数。这个时间的计算是发出 Request 到服务器返回 Response 的时间。

```py
import requests

r = requests.get('https://www.taobao.com', timeout=1)
print(r.status_code)
```

实际上请求分为两个阶段，即 connect（连接）和 read（读取）。
上面的设置 timeout 值将会用作 connect 和 read 二者的 timeout 总和。

如果要分别指定，就可以传入一个元组：

```py
r = requests.get('https://www.taobao.com', timeout=(5, 11))
```

#### 身份验证

```py
import requests
from requests.auth import HTTPBasicAuth

r = requests.get('http://localhost:5000', auth=HTTPBasicAuth('username', 'password'))
print(r.status_code)
```

如果用户名和密码正确的话，请求时就会自动认证成功，会返回 200 状态码，如果认证失败，则会返回 401 状态码。

当然如果参数都传一个 HTTPBasicAuth 类，就显得有点繁琐了，所以 Requests 提供了一个更简单的写法，可以直接传一个元组，它会默认使用 HTTPBasicAuth 这个类来认证。

```py
import requests

r = requests.get('http://localhost:5000', auth=('username', 'password'))
print(r.status_code)
```

#### Prepared Request

可以将 Request 表示为一个数据结构，Request 的各个参数都可以通过一个 Request 对象来表示:

```py
from requests import Request, Session

url = 'http://httpbin.org/post'
data = {
    'name': 'germey'
}
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
}
s = Session()
req = Request('POST', url, data=data, headers=headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
```

## 3.3 正则表达式

python中的re库提供了整个正则表达式的实现。

### 3.3.1 match()

传入要匹配的字符串以及正则表达式，检测是否匹配。

```py
import re

content = 'Hello 123 4567 World_This is a Regex Demo'
print(len(content))
result = re.match('^Hello\s\d\d\d\s\d{4}\s\w{10}', content)
print(result)
print(result.group())
print(result.span())

>>> 41
    <_sre.SRE_Match object; span=(0, 25), match='Hello 123 4567 World_This'>
    Hello 123 4567 World_This
    (0, 25)
```

#### 匹配

从字符串中提取一部分内容，可以使用()把我们想要的字符串括起来。

```py
import re

content = 'Hello 1234567 World_This is a Regex Demo'
result = re.match('^Hello\s(\d+)\sWorld', content)
print(result)
print(result.group())
print(result.group(1))
print(result.span())

>>> <_sre.SRE_Match object; span=(0, 19), match='Hello 1234567 World'>
    Hello 1234567 World
    1234567
    (0, 19)
```

其中group会输出完整匹配结果，而group(1)会输出第一个被()包围的匹配结果。

#### 通用匹配

正则表达式中.(点)可以匹配任意字符，*(星)代表匹配前面的字符无限次。

```py
import re

content = 'Hello 123 4567 World_This is a Regex Demo'
result = re.match('^Hello.*Demo$', content)
print(result)
print(result.group())
print(result.span())

>>> <_sre.SRE_Match object; span=(0, 41), match='Hello 123 4567 World_This is a Regex Demo'>
    Hello 123 4567 World_This is a Regex Demo
    (0, 41)
```

#### 贪婪与非贪婪

在通用匹配.*中遵循贪婪规则，尽可能多的去匹配字符。

```py
import re

content = 'Hello 1234567 World_This is a Regex Demo'
result = re.match('^He.*(\d+).*Demo$', content)
print(result)
print(result.group(1))

>>> <_sre.SRE_Match object; span=(0, 40), match='Hello 1234567 World_This is a Regex Demo'>
    7
```

由于第一个.*会尽可能多的匹配，所以\d+尽管是匹配多个数字，也只会匹配最后一个数字。

非贪婪规则，.*?可以避免这种情况。

* 如果非贪婪匹配放在结尾有可能匹配不到任何内容，因为它会尽可能少的匹配字符。

#### 修饰符

```py
import re

content = '''Hello 1234567 World_This
is a Regex Demo
'''

result = re.match('^He.*?(\d+).*?Demo$', content, re.S)
```

规则匹配是匹配除换行符之外的任意字符，当遇到换行符时就会匹配失败。

在match中传入re.S可以使匹配包括换行符在内的所有字符。

#### 转义匹配

如果匹配的字符串中包含.需要匹配，可以使用反斜杠来转义进行匹配，例如.可以使用\.

### 3.3.2 search()

在match()方法中，如果开头不匹配，整个匹配就会失败。

在search()中，会扫描整个字符串，找到匹配的开头，然后返回第一个成功匹配的结果。

```py
import re

content = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'
result = re.search('Hello.*?(\d+).*?Demo', content)
print(result)
```

### 3.3.3 findall()

search()方法中它只会返回匹配表达式的第一个内容，如果我们想要匹配所有内容，需要使用findall()。

### 3.3.4 sub()

除了提取，有时需要修改文本，比如想把文本中的所有数字都去掉，可使用sub()方法

```py
import re

content = '54aK54yr5oiR54ix5L2g'
content = re.sub('\d+', '', content)
print(content)

>>> aKyroiRixLg
```

第一个参数传入\d+匹配所有数字，第二个参数替换成字符串。

### 3.3.5 compile()

把正则字符串编译成正则表达式对象。

```py
import re

content1 = '2016-12-15 12:00'
content2 = '2016-12-17 12:55'
content3 = '2016-12-22 13:21'
pattern = re.compile('\d{2}:\d{2}')
result1 = re.sub(pattern, '', content1)
result2 = re.sub(pattern, '', content2)
result3 = re.sub(pattern, '', content3)
print(result1, result2, result3)
```

例如这里有三个日期，我们想分别将三个日期中的时间去掉，所以在这里我们可以借助于 sub() 方法，sub() 方法的第一个参数是正则表达式，但是这里我们没有必要重复写三个同样的正则表达式，所以可以借助于 compile() 方法将正则表达式编译成一个正则表达式对象，以便复用。